{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": ""
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Data Preprocessing\n\nThis notebook loads the raw data, preprocesses it, and saves the preprocessed data.\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import KNNImputer\nimport h5py\n\n# Load Data\ndef load_data(file_path):\n    with h5py.File(file_path, 'r') as f:\n        data = f['data'][:]\n    return data\n\ntrain_inputs = load_data('../data/train_multi_inputs.h5')\ntrain_targets = load_data('../data/train_multi_targets.h5')\ntest_inputs = load_data('../data/test_multi_inputs.h5')\n\n# Normalize Data\nscaler = StandardScaler()\ntrain_inputs_normalized = scaler.fit_transform(train_inputs)\ntest_inputs_normalized = scaler.transform(test_inputs)\n\n# Dimensionality Reduction\npca = PCA(n_components=100)\ntrain_inputs_pca = pca.fit_transform(train_inputs_normalized)\ntest_inputs_pca = pca.transform(test_inputs_normalized)\n\n# Impute Missing Values\nimputer = KNNImputer(n_neighbors=5)\ntrain_inputs_imputed = imputer.fit_transform(train_inputs_pca)\ntest_inputs_imputed = imputer.transform(test_inputs_pca)\n\n# Save Preprocessed Data\nnp.save('../data/train_inputs_preprocessed.npy', train_inputs_imputed)\nnp.save('../data/test_inputs_preprocessed.npy', test_inputs_imputed)\nnp.save('../data/train_targets.npy', train_targets)\n\nprint(\"Data preprocessing complete.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}